{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1 Introduction and Math Review\n",
    "__Math 3280: Data Mining__\n",
    "\n",
    "__Outline__\n",
    "1. Vectors\n",
    "2. Norms\n",
    "3. Dot Product\n",
    "4. Matrices and Linear Transformations\n",
    "5. Eigenvalues and Eigenvectors\n",
    "\n",
    "__Reading__ \n",
    "* Nield, Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors\n",
    "A vector is a mathematical tool that expresses both magnitude and direction.\n",
    "$$\\vec{y} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} \\qquad \\vec{z} = \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Norms (Measuring Distance)\n",
    "A norm is a way to calculate the length of a vector. Another way to say it is to measure the distance from the tail to the tip. But how do we measure the distance?\n",
    "\n",
    "Most common method is known at the Pythagorean Theorem,\n",
    "$$\\lVert x \\rVert = \\sqrt{x_0^2 + x_1^2 + x_2^2 + \\dots}$$\n",
    "\n",
    "This is actually one type of norm. We can create a simple equation to describe the norm:\n",
    "$$\\lVert x \\rVert_2 = \\sqrt{\\sum_i x_i^2}$$\n",
    "\n",
    "But there are other norms which measure the distance of a vector. We can generalize this equation for any norm:\n",
    "$$\\lVert x \\rVert_n = \\sqrt[n]{\\sum_i |x_i|^n}\\tag{$L_n$ norm}$$\n",
    "\n",
    "The most common norms are:\n",
    "1. $L_1$ norm (also known as the Manhattan Distance). This essentially finds the distance by following each component separately, as if you are walking from point A to point B on the streets of Manhattan.\n",
    "$$\\lVert x \\rVert_1 = \\sqrt[1]{\\sum_i |x_i|^1} = \\sum_i x_i = x_0+x_1+x_2+\\dots\\tag{$L_1$ norm}$$\n",
    "$$\\lVert y \\rVert_1 = 3 + 2 = 5$$\n",
    "\n",
    "2. $L_2$ norm (also known as both the Pythagorean Theorem and the Euclidean Distance).\n",
    "$$\\lVert x \\rVert_2 = \\sqrt[2]{\\sum_i x_i^2} = \\sqrt{x_0^2+x_1^2+x_2^2+\\dots}\\tag{$L_2$ norm}$$\n",
    "$$\\lVert y \\rVert_2 = \\sqrt{3^2 + 2^2} = \\sqrt{9+4} = \\sqrt{13} = 3.606$$\n",
    "\n",
    "3. $L_\\infty$ norm. With the infinity-norm, all components are raised to an extremely large power. Small components become insignificant to the largest component, so we end up with just the largest component, or the maximum.\n",
    "$$\\lVert x \\rVert_\\infty = \\sqrt[\\infty]{\\sum_i |x_i|^\\infty} = \\sqrt[\\infty]{max_i x_i^\\infty} = \\max_i x_i\\tag{$L_\\infty$ norm}$$\n",
    "$$\\lVert y \\rVert_\\infty = \\max\\{3,2\\} = 3$$\n",
    "\n",
    "Other norms are a measure of distance between the $L_2$ and $L_\\infty$ norms. For example, the $L_3$ norm would measure a distance, but the largest components are weighed a little more than the smaller components.\n",
    "$$\\lVert x \\rVert_3 = \\sqrt[3]{\\sum_i |x_i|^3} = \\sqrt[3]{x_0^3 + x_1^3 + x_2^3 + \\dots}\\tag{$L_3$ norm}$$\n",
    "$$\\lVert y \\rVert_3 = \\sqrt[3]{3^3 + 2^3} = \\sqrt[3]{27+8} = \\sqrt[3]{36} = 3.302$$\n",
    "\n",
    "The $L_{12}$ norm would be similar, but the weight of the largest components are weighed even heavier toward the distance than the $L_3$ norm.\n",
    "$$\\lVert x \\rVert_{12} = \\sqrt[12]{\\sum_i x_i^{12}} = \\sqrt[12]{x_0^{12} + x_1^{12} + x_2^{12} + \\dots}\\tag{$L_{12}$ norm}$$\n",
    "$$\\lVert y \\rVert_{12} = \\sqrt[12]{3^{12} + 2^{12}} = \\sqrt[12]{531441+4096} = \\sqrt[3]{535537} = 3.0019$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 norm: 5.0\n",
      "L2 norm: 3.605551275463989\n",
      "L3 norm: 3.2710663101885897\n",
      "L5 norm: 3.0751516574348225\n",
      "L10 norm: 3.005162300965063\n",
      "L15 norm: 3.000456245625036\n",
      "L20 norm: 3.0000451028565447\n",
      "L25 norm: 3.0000047521650317\n",
      "L30 norm: 3.0000005215081904\n"
     ]
    }
   ],
   "source": [
    "# This results in an overflow error if n is too large.\n",
    "def L_norm(x,n):\n",
    "    return pow(sum(x**n),1/n)\n",
    "\n",
    "import numpy as np\n",
    "a = np.array([3,2])\n",
    "\n",
    "for i in [1,2,3,5,10,15,20,25,30]:\n",
    "    print(f\"L{i} norm: {L_norm(a, i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, if a norm is given without a number, we default to the Euclidean norm (L2 norm).\n",
    "$$||\\vec{v}|| = ||\\vec{v}||_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, the $L_1$ and $L_2$ norms are easy. Even the $L_\\infty$ norm is simple enough to understand. But why would we want other norms, like the $L_3$ or $L_{12}$ norms?\n",
    "* Physical definition of Distance: Amount of space between two points\n",
    "* Reworded definition of Distance: Measure of how close two points are\n",
    "* Applied to Data Science: Measure of how close two measurements are\n",
    "\n",
    "With data, there are many ways to measure distance. We have only seen how to measure physical distance. But when it comes to data, there are many ways to measure distance, each one valuable in specific scenarios. Although we rarely use the $L_3$ or $L_{12}$ norms, there are occasions when it could be valuable.\n",
    "\n",
    "There are other ways to calculate distance:\n",
    "* Cosine Distance (measures angle between vectors)\n",
    "* Jaccard Distance (measures distance between two text documents)\n",
    "* Others...\n",
    "\n",
    "We will see these other measures of distance throughout the semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basis Vectors\n",
    "A basis vector is a vector of length 1 in the direction of a vector. The three most common basis vectors are the unit vectors in the x, y, and z directions:\n",
    "$$\\hat{i} = \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix} \\qquad \\hat{j} = \\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix} \\qquad \\hat{z} = \\begin{bmatrix}0 \\\\ 0 \\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "We can easily find a basis vector by dividing out the magnitude from any vector:\n",
    "$$\\hat{v} = \\frac{\\vec{v}}{||\\vec{v}||_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot Product\n",
    "We can multiply two vectors together many different ways. The most common method is the dot product. Just multiply the corresponding components for the two vectors together then add the results up.\n",
    "$$\\vec{v}\\cdot\\vec{w} = \\sum_i v_iw_i$$\n",
    "$$\\vec{y}\\cdot\\vec{z} = 3*1 + 2*4 = 3 + 8 = 11$$\n",
    "\n",
    "There is another way to calculate a dot product:\n",
    "$$\\vec{v}\\cdot\\vec{w} = \\vec{v}^T\\vec{w}$$\n",
    "$$\\vec{y}\\cdot\\vec{z} = \\vec{y}^T\\vec{z} = \\begin{bmatrix} 3 & 2\\end{bmatrix}\\begin{bmatrix}1 \\\\ 4\\end{bmatrix} = 3*1+2*4=3+8=11$$\n",
    "\n",
    "These formats are interchangeable. We'll see both forms throughout the semester.\n",
    "\n",
    "The dot product of a vector with itself is related to the Euclidean Norm of a vector:\n",
    "$$\\vec{v}\\cdot\\vec{v} = \\sum v_i^2 = ||\\vec{v}||^2_2 \\qquad \\to \\qquad ||\\vec{v}||_2 = \\sqrt{\\vec{v}\\cdot\\vec{v}}$$\n",
    "\n",
    "A third way to calaculate the dot product is using norms:\n",
    "$$\\vec{v}\\cdot\\vec{w} = ||v||_2||w||_2\\cos\\theta$$\n",
    "\n",
    "But what is the dot product? The dot product is often what we call a *projection*. It describes what component of vector $\\vec{v}$ is pointing in the same direction of vector $\\vec{w}$.\n",
    "> Examples:\n",
    "> * Project vector $\\vec{z}$ onto the x-axis and onto the y-axis\n",
    "> * Project vector $\\vec{z}$ onto vector $\\vec{y}$\n",
    "> \n",
    "> Consider demonstrating on Desmos\n",
    "\n",
    "* The result of the dot product is equal to (the component of vector $\\vec{v}$ in the direction of $\\vec{x}$) times (the magnitude of $\\vec{w})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Transformations (using Matrices)\n",
    "We can write a series of vectors together as a matrix:\n",
    "$$A = \\begin{bmatrix} a_{00} & a_{01} & \\dots & a_{0n} \\\\ a_{10} & a_{11} & \\dots & a_{1n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m0} & a_{m1} & \\dots & a_{mn}\\end{bmatrix}$$\n",
    "$$B = \\begin{bmatrix} 10 & 6 \\\\ 8 & 4\\end{bmatrix}$$\n",
    "\n",
    "Then, we can multiply the matrix by a vector (dot product of each row with the vector):\n",
    "$$B\\vec{y} = \\begin{bmatrix} 10 & 6 \\\\ 8 & 4\\end{bmatrix}\\begin{bmatrix} 3 \\\\ 2\\end{bmatrix} = \\begin{bmatrix} 10*3 + 6*2 \\\\ 8*3 + 4*2\\end{bmatrix} = \\begin{bmatrix} 42 \\\\ 32\\end{bmatrix}$$\n",
    "\n",
    "This can be useful to transform a vector to a new coordinate system. Our most traditional 2D coordinate system (known as the Cartesian Coordinate System) can be written as a matrix of two vectors: one for the x-axis and one for the y-axis.\n",
    "$$\\begin{bmatrix} 1 & 0 \\\\ 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "Each column would be a vector, and the combination of the vectors are known as __basis vectors__ which describe the coordinate system. Using this, we can create new coordinate systems. That is, we can measure a vector against any other set of vectors we want. For example, we can rotate the coordinate system 90 degrees counter-clockwise:\n",
    "$$R = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0\\end{bmatrix}$$\n",
    "\n",
    "Or we can use any other two vectors we want:\n",
    "$$K = \\begin{bmatrix} 3 & -1 \\\\ 4 & -2\\end{bmatrix}$$\n",
    "\n",
    "Then when we multiply a matrix by our vector, it transforms our vector onto the new coordinate system. For example, let's use our rotation matrix to rotate our vector 90 degrees counter-clockwise:\n",
    "$$R\\vec{y} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0\\end{bmatrix}\\begin{bmatrix} 3 \\\\ 2\\end{bmatrix} = \\begin{bmatrix} 0*3+(-1)*2 \\\\ 1*3+0*2\\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 3\\end{bmatrix}$$\n",
    "$$K\\vec{y} = \\begin{bmatrix} 3 & -1 \\\\ 4 & -2\\end{bmatrix}\\begin{bmatrix} 3 \\\\ 2\\end{bmatrix} = \\begin{bmatrix} 3*3+(-1)*2 \\\\ 4*3+(-2)*2\\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 8\\end{bmatrix}$$\n",
    "\n",
    "(Draw each transformation by drawing the vector on the Cartesian Coordinates, then beside it draw the vector relative to the new basis vectors.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvectors and Eigenvalues\n",
    "With some transformations, the direction of our vector doesn't change, but its length might. Such vectors are known as __eigenvectors__.\n",
    "$$A\\vec{x} = \\lambda\\vec{x}$$\n",
    "\n",
    "$\\lambda$ is a multiplier known as an __eigenvalue__. \n",
    "\n",
    "$$K\\vec{z} = \\begin{bmatrix} 3 & -1 \\\\ 4 & -2\\end{bmatrix}\\begin{bmatrix} 1 \\\\ 4\\end{bmatrix} = \\begin{bmatrix} 3*1+(-1)*4 \\\\ 4*1+(-2)*4\\end{bmatrix} = \\begin{bmatrix} -1 \\\\ -4\\end{bmatrix} = (-1)\\begin{bmatrix} 1 \\\\ 4\\end{bmatrix} = (-1)\\vec{z}$$\n",
    "\n",
    "So, $z = [1,4]$ is an eigenvector of $K$ with an eigenvalue of $-1$. (Draw the transformation.)\n",
    "\n",
    "Does $K$ have other eigenvalue/eigenvector pairs?\n",
    "$$\\begin{align*}K\\vec{x} &= \\lambda\\vec{x} \\\\\n",
    "  \\begin{bmatrix} 3 & -1 \\\\ 4 & -2\\end{bmatrix}\\begin{bmatrix} x_0 \\\\ x_1\\end{bmatrix} &= \\lambda \\begin{bmatrix} x_0 \\\\ x_1\\end{bmatrix}\\\\ \n",
    "  \\begin{bmatrix}3x_0-x_1 \\\\ 4x_0-2x_1\\end{bmatrix} &= \\begin{bmatrix} \\lambda x_0 \\\\ \\lambda x_1\\end{bmatrix} \\\\\n",
    "  3x_0-x_1 = \\lambda x_0 \\qquad&\\qquad 4x_0-2x_1=\\lambda x_1 \\\\\n",
    "  (3-\\lambda)x_0 - x_1 = 0 \\qquad&\\qquad 4x_0-(2+\\lambda)x_1 = 0\\\\\n",
    "  x_1 = (3-\\lambda)x_0 \\qquad & \\qquad 4x_0-(2+\\lambda)(3-\\lambda)x_0 = 0 \\\\\n",
    "  &\\qquad 4 - (6+\\lambda-\\lambda^2) = 0 \\\\\n",
    "  &\\qquad \\lambda^2 - \\lambda - 2 = 0 \\\\\n",
    "  &\\qquad \\lambda = \\frac{1\\pm\\sqrt{1-4(1)(-2)}}{2(1)} \\\\\n",
    "  &\\qquad \\lambda = \\{2,-1\\}\n",
    "\\end{align*}$$\n",
    "\n",
    "So, we have two eigenvalues: $\\lambda = 2$ and $\\lambda = -1$. We already saw the eigenvalue $\\lambda=-1$ has an eigenvector of $\\vec{x}=\\begin{bmatrix}1\\\\4\\end{bmatrix}$. What's the eigenvector for $\\lambda = 2$?\n",
    "$$\\begin{align*}\n",
    "  3x_0-x_1 = 2x_0 \\qquad&\\qquad 4x_0-2x_1=2x_1 \\\\\n",
    "  x_0-x_1 = 0 \\qquad&\\qquad 4x_0-4x_1=0 \\\\\n",
    "  x_0 = x_1 \\qquad&\\qquad x_0=x_1\n",
    "\\end{align*}$$\n",
    "All we know is that $x_0=x_1$, which means,\n",
    "$$\\vec{x} = \\begin{bmatrix}x_0\\\\x_1\\end{bmatrix} = \\begin{bmatrix}x_0\\\\x_0\\end{bmatrix} = x_0\\begin{bmatrix}1\\\\1\\end{bmatrix}$$\n",
    "The eigenvector for $\\lambda=2$ is $\\vec{x}=\\begin{bmatrix}1\\\\1\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

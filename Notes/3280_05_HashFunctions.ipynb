{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 Hash Functions\n",
    "__Math 3280: Data Mining__\n",
    "\n",
    "__Outline__\n",
    "1. Basics of Hash Functions\n",
    "2. When two entries get the same index\n",
    "3. Tips on Hash Functions\n",
    "4. Hash Functions with Text\n",
    "5. Intro to MapReduce and Supercomputers\n",
    "\n",
    "__Reading__ \n",
    "* Leskovec, Section 1.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Hash Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we need to search for a particular value, we could simply go through all the values until we find the one we want. This is called a __linear search__. For small datasets, this works just fine. But for large datasets, this is inefficient.\n",
    "\n",
    "A __hash function__ takes some key value related to the data and produces a __bucket number__, or a __hash-key__. That is, we take something intuitive about the data (ID, name, timestamp,...) and do some calculation on it to determine what bucket, or place in our array, the data should be stored. Then when we want to recall that data, we do the same calculation, and we know exactly where that data is stored.\n",
    "\n",
    "The following three examples demonstrate how one common hash function works, and presents a potential issue.\n",
    "\n",
    "*Example 1*:\n",
    "> You have data for 10 patients that you want to store in the database.\n",
    "> * Their IDs are:\n",
    ">   * [100, 186, 152, 199, 103, 127, 175, 131, 114, 148]\n",
    "> * To determine the bucket to store the data in (the hash-key), take the modulus of each ID with the number of elements (10)\n",
    "> $$f_h(x) = x \\% n$$\n",
    ">   * [0, 6, 2, 9, 3, 7, 5, 1, 4, 8]\n",
    "> * Store the data:\n",
    ">   * `ID = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]`\n",
    ">   * `ID = [100, ___, ___, ___, ___, ___, ___, ___, ___, ___]`\n",
    ">   * `ID = [100, ___, ___, ___, ___, ___, 186, ___, ___, ___]`\n",
    ">   * `ID = [100, ___, 152, ___, ___, ___, 186, ___, ___, ___]`\n",
    ">   * `ID = [100, ___, 152, ___, ___, ___, 186, ___, ___, 199]`\n",
    ">   * `ID = [100, ___, 152, 103, ___, ___, 186, ___, ___, 199]`\n",
    ">   * `ID = [100, ___, 152, 103, ___, ___, 186, 127, ___, 199]`\n",
    ">   * `ID = [100, ___, 152, 103, ___, 175, 186, 127, ___, 199]`\n",
    ">   * `ID = [100, 131, 152, 103, ___, 175, 186, 127, ___, 199]`\n",
    ">   * `ID = [100, 131, 152, 103, 114, 175, 186, 127, ___, 199]`\n",
    ">   * `ID = [100, 131, 152, 103, 114, 175, 186, 127, 148, 199]`\n",
    "> * If you want patient 186, take the modulus $f_h(186) = 186 \\% 10 = 6$. The data is in bucket 6 for all lists.\n",
    ">   * `ID[6] = 186`, `name[6]`, `weight[6]`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When two entries get the same index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, our hash function will cause two or more entries to receive the same hash-key. For example, $f_h(114) = 114\\%10 = 4$ and $f_h(124) = 124\\%10 = 4$. \n",
    "\n",
    "We start by going to the bucket indicated by our hash-key, just as before. If that bucket is already filled, move to the next index. Sometimes, you may have to advance multiple indices before finding an empty bucket.\n",
    "\n",
    "When we recall the information, then the index from our hash-key becomes a starting point for a linear search. If the correct entry is in the bucket from our calculation, then no search is required. If the correct entry is not in the bucket from our calculation, then we look at the next bucket, then the next, and so on until we find the right information.\n",
    "\n",
    "*Example 2*:\n",
    "\n",
    "This example is the same as example 1, but notice that some of the calculations repeat bucket numbers:\n",
    "> You have data for 10 patients that you want to store in the database.\n",
    "> * Their IDs are:\n",
    ">   * [245, 287, 261, 295, 233, 209, 276, 284, 260, 221]\n",
    "> * To determine the bucket to store the data in, take the modulus of each ID with the number of elements (10)\n",
    ">   * [5, 7, 1, 5, 6, 3, 6, 1, 4, 8]\n",
    "> * Store the data:\n",
    ">   * `ID = [___, ___, ___, ___, ___, 245, ___, ___, ___, ___]`\n",
    ">   * `ID = [___, ___, ___, ___, ___, 245, ___, 287, ___, ___]`\n",
    ">   * `ID = [___, 261, ___, ___, ___, 245, ___, 287, ___, ___]`\n",
    "> * The next is 295 going into bucket 5. But bucket 5 is already filled. So, fill the next bucket.\n",
    ">   * `ID = [___, 261, ___, ___, ___, 245, 295, 287, ___, ___]`\n",
    ">   * `ID = [___, 261, ___, 233, ___, 245, 295, 287, ___, ___]`\n",
    ">   * `ID = [___, 261, ___, 233, ___, 245, 295, 287, ___, 209]`\n",
    "> * The next is 276 going into bucket 6. But bucket 6 is already filled. So, go to the next bucket, but that is also filled. Just keep going and fill the next available bucket.\n",
    ">   * `ID = [___, 261, ___, 233, ___, 245, 295, 287, 276, 209]`\n",
    ">   * `ID = [___, 261, ___, 233, 284, 245, 295, 287, 276, 209]`\n",
    ">   * `ID = [260, 261, ___, 233, 284, 245, 295, 287, 276, 209]`\n",
    "> * The next is 221 going into bucket 1. But bucket 1 is already filled. So, fill the next bucket.\n",
    ">   * `ID = [260, 261, 221, 233, 284, 245, 295, 287, 276, 209]`\n",
    "> * If you want patient 233, take the modulus $233 mod 10 = 3$. The data is in bucket 3 for all lists.\n",
    ">   * `ID[3] = 233`, `name[3]`, `weight[3]`, ...\n",
    "> * If you want patient 276, take the modulus $276 mod 10 = 6$. But this time, the data isn't in bucket 6. Go to bucket 6 and start a linear search from there.\n",
    ">   * `ID[6] = 295`\n",
    ">   * `ID[7] = 287`\n",
    ">   * `ID[8] = 276` is a match!\n",
    ">   * The data is in bucket 8 for all lists.\n",
    ">   * `ID[8] = 276`, `name[8]`, `weight[8]`, ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips on Hash Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the possibility of overlapping data from our hash function, there are a few tips that help to reduce this possibility.\n",
    "1. Make the array larger than it needs to be\n",
    "    * If the array is larger, than that gives more possible results, reducing the chance for repeated hash-keys\n",
    "    * If there are repeated hash-keys from $f_h(x)$, then there is more likely space close to the result, reducing the length of the linear search if it's needed\n",
    "2. Make the array size ($n$) a prime number\n",
    "    * If there is a prime number of bins, then the chance of repeated results decreases\n",
    "    * Choosing ($n$) such that it has common factors with most hash-keys, then the possible hash-keys result in nonrandom distribution into buckets - so a prime number of buckets is preferred\n",
    "\n",
    "    > Suppose your population is only contained of even numbers. If $n=10$, then the only buckets that can be filled normally are $0, 2, 4, 6,$ and $8$. However, if we choose $n=11$, then the even integers create an equal 1/11 probability for each bucket.\n",
    "\n",
    "    * Be sure to consider the case when the prime number $n$ is a factor in most values of your population. If this is the case, just choose a different prime number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Functions with Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are dealing with text, we have to find a way to convert text into numberical values. A simple example would be to convert each letter in the text into its appropriate ASCII code.\n",
    "\n",
    "*Example 3*:\n",
    "\n",
    "In this example, we use names instead of IDs.\n",
    "> You have data for 5 patients that you want to store in the database.\n",
    "> * Their names are:\n",
    ">   * [Jon, Sue, Sam, Dan, Ted]\n",
    "> * Create a numberical value by adding the ASCII codes for each character in the name. Then take the modulus of that result with the number of patients (5).\n",
    ">   * 'Jon' = 74 + 111 + 110 = 295 --> 295 mod 5 = 0\n",
    ">   * 'Sue' = 83 + 117 + 101 = 301 --> 301 mod 5 = 1\n",
    ">   * 'Sam' = 83 +  97 + 109 = 289 --> 289 mod 5 = 4\n",
    ">   * 'Dan' = 68 +  97 + 110 = 275 --> 275 mod 5 = 0\n",
    ">   * 'Ted' = 84 + 101 + 100 = 285 --> 285 mod 5 = 0\n",
    "> * Store the data:\n",
    ">   * `ID = [Jon, ___, ___, ___, ___]`\n",
    ">   * `ID = [Jon, Sue, ___, ___, ___]`\n",
    ">   * `ID = [Jon, Sue, ___, ___, Sam]`\n",
    ">   * `ID = [Jon, Sue, Dan, ___, Sam]`\n",
    ">   * `ID = [Jon, Sue, Dan, Ted, Sam]`\n",
    "\n",
    "Notice how in this last example, finding the record for Ted is almost as many tests as just doing a linear search. Certain datapoints could have that issue. But for the most part, this is a very straightforward hash function that simplifies the search process. On the whole, the number of calculations needed to find a name has dropped drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to MapReduce and Supercomputers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we look at the computer requirements when dealing with big data.\n",
    "\n",
    "When dealing with large computations, such as large-scale models, one computer will not be enough.\n",
    "> As an undergraduate, I created a model of air pollution in North Salt Lake. It was a 24-hour model covering a 100-km^2 area. It took well over 1 hour on my computer to get the results. Imagine how much more time it would have taken as a 3-Dimensional model covering the entire planet... By the time my computer finished a forecast model, the event being forecasted would have happened weeks ago.\n",
    "\n",
    "To handle large computers, we utilize __parallel processing__, where several processors are linked together and work on parts of the problem simultaneously. This helps the calculations to complete in far less time.\n",
    "* Each processor is called a __node__.\n",
    "* The collection of nodes is called a __supercomputer__.\n",
    "\n",
    "In data science, however, we are not only dealing with large computations, but with large amounts of data as well. \n",
    "* For example, large-scale Web services, such as Google or Amazon, are continually dealing with large amounts of data and customer interactions.\n",
    "\n",
    "To handle this, we use not only the processors on each node, but the storage space as well. \n",
    "* These systems are known as __computing clusters__.\n",
    "* The software to manage the data and queries is a __distributed file system__.\n",
    "\n",
    "### How to cool a Supercomputer or Computing Cluster\n",
    "We have a large issue with computers burning out do to excessive heat. With that many computers running all at once, the temperature in the room rises rapidly.\n",
    "* Most computing clusters are stored in independent rooms with extremely powerful air conditioning units\n",
    "  * If the AC goes out, the computer must be shut down immediately\n",
    "* Some large companies have developed other methods\n",
    "  * Immersion Cooling - Using fluids instead of fans and air as a coolant\n",
    "    * https://submer.com/blog/what-is-immersion-cooling/\n",
    "    * https://en.wikipedia.org/wiki/Aquasar\n",
    "    * https://www.datacenterknowledge.com/sustainability/submerged-supercomputer-named-world-s-most-efficient-system-in-green-500"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--DmgKU1M-z5"
      },
      "source": [
        "# Lecture 7a PySpark\n",
        "__Math 3280: Data Mining__\n",
        "\n",
        "__Outline__\n",
        "1. Example of using PySpark\n",
        "\n",
        "__Reading__ \n",
        "* Rioux, Chapter 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvuilPVyM4FB"
      },
      "source": [
        "Most data-driven applications are built in three steps, or a simple ETL:\n",
        "1. Loading (or *__e__xtracting*)\n",
        "2. __T__ransforming\n",
        "3. Exporting (or *__l__oading* into the bigger system)\n",
        "\n",
        "First, set up Spark in Google CoLab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "N2D8B8S9F-L5",
        "outputId": "a510072f-5909-4aae-d4cd-96f8f80b3195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "68 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/usr/local/lib/python3.10/dist-packages/pyspark'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kgixzR6Nwej"
      },
      "source": [
        "Now, we create a session in Spark. For this we use an entry point called `SparkContext`, a liason between our Python terminal and the Spark cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5ku2NSIFQ5VJ"
      },
      "outputs": [],
      "source": [
        "# Create a `SparkSession` entry point from scratch\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = {SparkSession\n",
        "         .builder\n",
        "         .appName(\"Analyzing the vocabulary of books.\")\n",
        "         .getOrCreate()\n",
        "         }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "R4SoDcUbOG1B",
        "outputId": "d00ea994-ba15-4021-ed25-913f5622a99f"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'set' object has no attribute 'sparkContext'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a98af5fecdd2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'sparkContext'"
          ]
        }
      ],
      "source": [
        "spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFgrOqJ0OEtW"
      },
      "outputs": [],
      "source": [
        "spark.sparkContext.setLogLevel(\"FATAL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC6uvwJ5KT_5"
      },
      "source": [
        "With our session started, let's plan out how we're going to tackle the problem:\n",
        "* __Goal__: Read through thousands of books to find most commonly used words\n",
        "  * Gather lots of books from the free domain\n",
        "  * We will test the program on just one book in the free domain: *Pride and Prejudice*\n",
        "\n",
        "### __E__TL: Extract the data\n",
        "Where will the data go once loaded?\n",
        "* The RDD (Resilient Distributed Dataset)\n",
        "* A DataFrame\n",
        "\n",
        "![RDD vs DF](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781617297205/files/OEBPS/Images/02-02.png)\n",
        "\n",
        "A DataFrame is essentially a stricter version of the RDD. An RDD uses a relation (a DataFrame) organized in tuples (rows of a DF) and attributes (columns of a DF).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "yfoBP9D-LcjK",
        "outputId": "0988e937-011e-4608-9011-b90a11ce7e2e"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'set' object has no attribute 'read'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5a6a17a175c3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'read'"
          ]
        }
      ],
      "source": [
        "dir(spark.read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLIXUJfgLdVI"
      },
      "outputs": [],
      "source": [
        "spark.read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkDo2yLMLdMS"
      },
      "outputs": [],
      "source": [
        "book = spark.read.text('/content/1342-0.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp3a_1ESLdDb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TqYKRmn1GGb0",
        "outputId": "42f55bd4-378d-4576-aee8-abacaf81e8d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Ign:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,308 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,150 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,097 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 6,684 kB in 2s (2,825 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "70 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| the| 4480|\n",
            "|  to| 4218|\n",
            "|  of| 3711|\n",
            "| and| 3504|\n",
            "| her| 2199|\n",
            "|   a| 1982|\n",
            "|  in| 1909|\n",
            "| was| 1838|\n",
            "|   i| 1749|\n",
            "| she| 1668|\n",
            "|that| 1487|\n",
            "|  it| 1482|\n",
            "| not| 1427|\n",
            "| you| 1300|\n",
            "|  he| 1296|\n",
            "|  be| 1257|\n",
            "| his| 1247|\n",
            "|  as| 1174|\n",
            "| had| 1170|\n",
            "|with| 1092|\n",
            "+----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "###  Install in Google CoLab  ###\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "###  Create SparkSession  ###\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark\n",
        "\n",
        "###  Transforming  ###\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\n",
        "    \"Counting word occurences from a book.\"\n",
        ").getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "##########################################################\n",
        "#####   TOKENIZATION - LEARN MORE ABOUT THIS IN NLP  #####\n",
        "\n",
        "# If you need to read multiple text files, replace `1342-0` by `*`.\n",
        "results = (\n",
        "    spark.read.text(\"/content/1342-0.txt\")\n",
        "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
        "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
        "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
        "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
        "    .where(F.col(\"word\") != \"\")\n",
        "    .groupby(F.col(\"word\"))\n",
        "    .count()\n",
        ")\n",
        "\n",
        "results.orderBy(\"count\", ascending=False).show(20)\n",
        "#results.orderBy(\"word\", ascending=True).show(20)\n",
        "#results.coalesce(1).write.csv(\"./results_single_partition.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK0wLjf0Gy92"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col,\n",
        "    explode,\n",
        "    lower,\n",
        "    regexp_extract,\n",
        "    split,\n",
        ")\n",
        "\n",
        "spark = SparkSession.builder.appName(\n",
        "    \"Analyzing the vocabulary of Pride and Prejudice.\"\n",
        ").getOrCreate()\n",
        "\n",
        "book = spark.read.text(\"/content/1342-0.txt\")\n",
        "#book = spark.read.text(\"https://raw.githubusercontent.com/jonesberg/DataAnalysisWithPythonAndPySpark-Data/trunk/gutenberg_books/1342-0.txt\")\n",
        "\n",
        "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
        "\n",
        "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
        "\n",
        "words_lower = words.select(lower(col(\"word\")).alias(\"word\"))\n",
        "\n",
        "words_clean = words_lower.select(\n",
        "    regexp_extract(col(\"word\"), \"[a-z']*\", 0).alias(\"word\")\n",
        ")\n",
        "\n",
        "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
        "\n",
        "results = words_nonull.groupby(col(\"word\")).count()\n",
        "\n",
        "results.orderBy(\"count\", ascending=False).show(10)\n",
        "\n",
        "results.coalesce(1).write.csv(\"./simple_count_single_partition.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5qMBaaJhM9g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

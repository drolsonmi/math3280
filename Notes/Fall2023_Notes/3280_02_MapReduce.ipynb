{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced18847-16cd-492a-a57d-8fb26c042881",
   "metadata": {},
   "source": [
    "# 02 MapReduce\n",
    "__Math 3280 - Data Mining__ : Snow College : Dr. Michael E. Olson\n",
    "* Leskovec, Chapter 2\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0937ebc0-9cd5-4895-bda7-0335ace5c02d",
   "metadata": {},
   "source": [
    "In this chapter, we look at the computer requirements when dealing with big data.\n",
    "\n",
    "When dealing with large computations, such as large-scale models, one computer will not be enough.\n",
    "> As an undergraduate, I created a model of air pollution in North Salt Lake. It was a 24-hour model covering a 100-km^2 area. It took well over 1 hour on my computer to get the results. Imagine how much more time it would have taken as a 3-Dimensional model covering the entire planet... By the time my computer finished a forecast model, the event being forecasted would have happened weeks ago.\n",
    "\n",
    "To handle large computers, we utilize __parallel processing__, where several processors are linked together and work on parts of the problem simultaneously. This helps the calculations to complete in far less time.\n",
    "* Each processor is called a __node__.\n",
    "* The collection of nodes is called a __supercomputer__.\n",
    "\n",
    "In data science, however, we are not only dealing with large computations, but with large amounts of data as well. \n",
    "* For example, large-scale Web services, such as Google or Amazon, are continually dealing with large amounts of data and customer interactions.\n",
    "\n",
    "To handle this, we use not only the processors on each node, but the storage space as well. \n",
    "* These systems are known as __computing clusters__.\n",
    "* The software to manage the data and queries is a __distributed file system__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce85953-c034-49a2-80be-7d36c4443ea2",
   "metadata": {},
   "source": [
    "## Cluster Computing and the Distributed File System\n",
    "* Each processor/storage unit is called a __node__.\n",
    "* Each node is installed on a __rack__.\n",
    "  * There are often 8-64 nodes on a rack.\n",
    "  * Each node on that rack is connected by a localized network - typically a gigabit ethernet.\n",
    "* A collection of several racks is a __cluster__.\n",
    "  * Several racks are then connected by another level of network or a switch.\n",
    "\n",
    "In order to get all the information from the racks to work with each other, they need more bandwidth than the rack itself has. We will learn about how these are used soon. First, let's look at the hardware challenges.\n",
    "\n",
    "All hardware eventually fails. With heavy usage, it will fail faster. \n",
    "* In large-scale services, one node can last about 3 years (a little more than 1000 days)\n",
    "* If I have a server of 1000 nodes, that means that on average, 1 node will fail every day\n",
    "* A server at Google may have a million nodes, which means there are about 1000 nodes that fail every day\n",
    "\n",
    "With so many failures, we have to ensure no disruption in data or in calculations if the failure happens while the program is running. To ensure this happens, there are two requirements:\n",
    "1. Files must be stored redundantly\n",
    "2. Computations must be divided into smaller tasks\n",
    "    * If one task fails, then only that one task needs to be restarted, not the entire program\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf078b57-509e-41ee-a413-b0ed3ea7f3f2",
   "metadata": {},
   "source": [
    "### The Distributed File System Organization\n",
    "A __distributed file system (DFS)__ works by dividing the data file into separate pieces and copying them.\n",
    "1. Files are divided into __chunks__, typically 64 MB\n",
    "    * Size can be determined by the user\n",
    "2. Each chunk is saved on different nodes\n",
    "3. Each chunk is the replicated and saved on different nodes, perhaps 3 times\n",
    "    * Number of copies can be determined by the user\n",
    "    * The nodes holding the copies should be on different racks so copies aren't all lost if a rack fails\n",
    "4. A __master node__ (or name node) tracks the location of all chunks so retrieval is simplified\n",
    "    * The master node is also replicated\n",
    "\n",
    "A DFS is often used when,\n",
    "* individual files are large (terabytes), and\n",
    "* files are rarely updated\n",
    "\n",
    "There is no need for files to be distributed if they are small. And if any file is frequently updated, then the process becomes very complicated. So, this may be a good system for data on a global scale, but wouldn't work well for Amazon who has changes in inventory and prices daily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90619b-77a0-47f8-a9c7-957873f3a454",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "__MapReduce__ is the style of computing that is used to implement the DFS methodology. There are many different implementations:\n",
    "* (GFS) Google File System - The original\n",
    "* (HDFS) Hadoop DFS - Open-source, distributed by the Apache Software Foundation\n",
    "* Spark\n",
    "* Colossus - An improved version of GFS\n",
    "\n",
    "The MapReduce process only involves two functions: *Map* and *Reduce*. The process is as follows. We'll follow the process with an example of counting the number of words.\n",
    "1. *Map tasks* are given one or more chunks from the DFS and matches it into key-value pairs\n",
    "    * Each map task looks for the words $w_1$, $w_2$, etc.\n",
    "    * The key-value pair would be <$w_1, v_1$>, <$w_2,v_2$>, etc.\n",
    "    * The result is a list of all key-value pairs <$w_i, v_i$> for all documents\n",
    "2. A __master controller__ sorts and groups these key-value pairs and assigns them to a *Reduce task* \n",
    "    * All word pairs are sorted: <$w_1, v_1$>, <$w_1, v_x$>, ..., <$w_2, v_2$>, <$w_2, v_y$>, ..., <$w_3, v_3$>, <$w_3, v_z$>, ... \n",
    "    * These pairs are then grouped as <$w_1, [v_1,...]>$, <$w_2,[v_2,...]$>, <$w_3, [v_3,...]$>, ... ,\n",
    "    * Each group is then assigned to a *Reduce Task* for the final computation\n",
    "3. *Reduce Tasks* work with one key at a time, combining the values associated with that key in some way\n",
    "    * All pairs with $w_1$ are given to one reduce task, $w_2$ to another, etc.\n",
    "    * If the tasks are small enough, multiple tasks can be assigned to the same node\n",
    "      * Input to ReduceTask1: <$w_1, [v_1,...]$>\n",
    "      * Input to ReduceTask2: <$w_2, [v_2,...]$>\n",
    "      * Input to ReduceTask3: <$w_3, [v_3,...]$>, <$w_4, [v_4, ...]$>\n",
    "      * ...\n",
    "    * Add all the values together\n",
    "      * Output from ReduceTask1: <$w_1, x_1$>\n",
    "      * Output from ReduceTask2: <$w_2, x_2$>\n",
    "      * Output from ReduceTask3: <$w_3, x_3$>, <$w_4, x_4$>\n",
    "    * A __combiner__ is a reduce function that is associative and commutative\n",
    "\n",
    "The Master Controller handles the process by,\n",
    "1. Assigning nodes in the cluster to complete either a Map Task or a Reduce Task, never both\n",
    "    * Nodes assigned to complete Map or Reduce Tasks are known as __workers__\n",
    "2. Tracks the status of workers\n",
    "    * When workers report that they are done, the Master Controller can schedule a new task to that node\n",
    "\n",
    "Example of a MapReduce process given in PowerPoint.\n",
    "      \n",
    "### Node Failure\n",
    "What happens if a node fails in the middle?\n",
    "* Best case scenario: only a single map task or reduce task needs to be restarted\n",
    "* Worst cast scenario: the node at which the Master is executing fails, and the entire MapReduce job needs to be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735fc69-9053-4dfd-a6ab-8f4db49233d1",
   "metadata": {},
   "source": [
    "## Algorithms using MapReduce\n",
    "The original use for MapReduce was to complete Matrix-Vector multiplication. We will look at the calculation of $M\\vec{v}=\\vec{x}$ where $M$ is a $p\\times q$ matrix, $\\vec{v}$ is a vector with $q$ elements, and $\\vec{x}$ is the result of the calculation, a vector with $p$ elements.\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "  m_{00} & m_{01} & m_{02} & \\dots  & m_{0q} \\\\\n",
    "  m_{10} & m_{11} & m_{12} & \\dots  & m_{1q} \\\\\n",
    "  m_{20} & m_{21} & m_{22} & \\dots  & m_{2q} \\\\\n",
    "  \\vdots &        &        & \\ddots & \\vdots \\\\\n",
    "  m_{p0} & m_{p1} & m_{p2} & \\dots  & m_{pq} \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "  v_0 \\\\ v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_q\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "  x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_p\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Note that MapReduce is not helpful when $p$ is small enough ($p=100$) to be done on individual computers. MapReduce is more useful when $M$ is so large that it doesn't fit into the memory of a single node.\n",
    "\n",
    "### Matrix-Vector Multiplication with small vectors\n",
    "Look first at the case when $q$ is small enough that $\\vec{v}$ fits into memory. But $M$ is still too large.\n",
    "* Divide $M$ into sections with multiple rows\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "  m_{00} & m_{01} & m_{02} & \\dots  & m_{0q} \\\\\n",
    "  m_{10} & m_{11} & m_{12} & \\dots  & m_{1q} \\\\\n",
    "  m_{20} & m_{21} & m_{22} & \\dots  & m_{2q} \\\\\n",
    "  ---    & ---    & ---    & ---    & ---    \\\\\n",
    "  m_{30} & m_{31} & m_{32} & \\dots  & m_{3q} \\\\\n",
    "  m_{40} & m_{41} & m_{42} & \\dots  & m_{4q} \\\\\n",
    "  m_{50} & m_{51} & m_{52} & \\dots  & m_{5q} \\\\\n",
    "  ---    & ---    & ---    & ---    & ---    \\\\\n",
    "  m_{60} & m_{61} & m_{62} & \\dots  & m_{6q} \\\\\n",
    "  m_{70} & m_{71} & m_{72} & \\dots  & m_{7q} \\\\\n",
    "  m_{80} & m_{81} & m_{82} & \\dots  & m_{8q} \\\\\n",
    "  ---    & ---    & ---    & ---    & ---    \\\\\n",
    "  \\vdots &        &        & \\ddots & \\vdots \\\\\n",
    "  m_{p0} & m_{p1} & m_{p2} & \\dots  & m_{pq} \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "  v_0 \\\\ v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_q\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "* The vector $\\vec{v}$ is stored in the memory of each Map worker\n",
    "* The first Map worker is assigned this first section, and so on\n",
    "* Map Task: Make a list of all key value pairs <$i,m_{ij}, v_j$>\n",
    "  * <$0,m_{00},v_0$>, <$0,m_{01},v_1$>, <$0,m_{02},v_2$>, ..., <$0,m_{0q},v_q$>\n",
    "  * <$1,m_{10},v_0$>, <$1,m_{11},v_1$>, <$1,m_{12},v_2$>, ..., <$1,m_{1q},v_q$>\n",
    "  * ...\n",
    "  * <$p,m_{p0},v_0$>, <$p,m_{p1},v_1$>, <$p,m_{p2},v_2$>, ..., <$p,m_{pq},v_q$>\n",
    "* Grouping: Take the product $m_{ij}v_j$ and group the results with the same index $i$\n",
    "  * <$0, [m_{00}v_0, m_{01}v_1, m_{02}v_2, ... , m_{0q}v_q]$>\n",
    "  * <$1, [m_{10}v_0, m_{11}v_1, m_{12}v_2, ... , m_{1q}v_q]$>\n",
    "  * <$2, [m_{20}v_0, m_{21}v_1, m_{22}v_2, ... , m_{2q}v_q]$>\n",
    "  * ...\n",
    "  * <$p, [m_{p0}v_0, m_{p1}v_1, m_{p2}v_2, ... , m_{pq}v_q]$>\n",
    "* Reduce Task: Find the sum of all elements in each group\n",
    "  * <$0, x_0$>\n",
    "  * <$1, x_1$>\n",
    "  * <$2, x_2$>\n",
    "  * ...\n",
    "  * <$p, x_p$>\n",
    "* Final Output:\n",
    "$$\\vec{x} = \\begin{bmatrix}\n",
    "  x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_p\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc29cf-e89a-49e8-9f3d-a61f0843e7eb",
   "metadata": {},
   "source": [
    "### Matrix-Vector Multiplication with large vectors\n",
    "The previous works well when $q$ is small enough for $\\vec{v}$ to fit in memory. But if $q$ is too large, then we can't do it as we did before. However, we can add one step which will allow us to continue this method: divide $\\vec{v}$ into sections as well:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "  m_{00} & m_{01} & | & m_{02} & m_{03} & | & m_{04} & m_{05} & | & \\dots  & m_{0q} \\\\\n",
    "  m_{10} & m_{11} & | & m_{12} & m_{13} & | & m_{14} & m_{15} & | & \\dots  & m_{1q} \\\\\n",
    "  m_{20} & m_{21} & | & m_{22} & m_{23} & | & m_{24} & m_{25} & | & \\dots  & m_{2q} \\\\\n",
    "  ---    & ---    & | & ---    & ---    & | & ---    & ---    & | & ---    & ---    \\\\\n",
    "  m_{30} & m_{31} & | & m_{32} & m_{33} & | & m_{34} & m_{35} & | & \\dots  & m_{3q} \\\\\n",
    "  m_{40} & m_{41} & | & m_{42} & m_{43} & | & m_{44} & m_{45} & | & \\dots  & m_{4q} \\\\\n",
    "  m_{50} & m_{51} & | & m_{52} & m_{53} & | & m_{54} & m_{55} & | & \\dots  & m_{5q} \\\\\n",
    "  ---    & ---    & | & ---    & ---    & | & ---    & ---    & | & ---    & ---    \\\\\n",
    "  m_{60} & m_{61} & | & m_{62} & m_{63} & | & m_{64} & m_{65} & | & \\dots  & m_{6q} \\\\\n",
    "  m_{70} & m_{71} & | & m_{72} & m_{73} & | & m_{74} & m_{75} & | & \\dots  & m_{7q} \\\\\n",
    "  m_{80} & m_{81} & | & m_{82} & m_{83} & | & m_{84} & m_{85} & | & \\dots  & m_{8q} \\\\\n",
    "  ---    & ---    & | & ---    & ---    & | & ---    & ---    & | & ---    & ---    \\\\\n",
    "  \\vdots &        & | & \\vdots &        & | & \\vdots &        & | & \\ddots & \\vdots \\\\\n",
    "  m_{p0} & m_{p1} & | & m_{p2} & m_{p3} & | & m_{p4} & m_{p5} & | & \\dots  & m_{pq}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "  v_0 \\\\ v_1 \\\\ --- \\\\ v_2 \\\\ v_3 \\\\ --- \\\\ v_4 \\\\ v_5 \\\\ --- \\\\ \\vdots \\\\ v_q\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "* Map Task 1 will get the 1st 2 elements of rows 0-2 in $M$ and the 1st 2 elements of $\\vec{v}$\n",
    "* Map Task 2 will get the 2nd 2 elements of rows 0-2 in $M$ and the 2nd 2 elements of $\\vec{v}$\n",
    "* Map Task 3 will get the 3rd 2 elements of rows 0-2 in $M$ and the 3rd 2 elements of $\\vec{v}$\n",
    "* and so on until rows 0-2 are complete\n",
    "* The next Map Task will get the 1st 2 elements of rows 3-5 in $M$ and the 1st 2 elements of $\\vec{v}$\n",
    "* The next Map Task will get the 2nd 2 elements of rows 3-5 in $M$ and the 2nd 2 elements of $\\vec{v}$\n",
    "* and so on...\n",
    "\n",
    "Once the mapping task is done, the grouping and reduce tasks are the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e462bdb-d322-43aa-9bd0-95fcb320b4be",
   "metadata": {},
   "source": [
    "-----\n",
    "## Homework\n",
    "1. Exercise 2.2.1 (a,b)\n",
    "2. Exercise 2.3.1\n",
    "3. Exercise 2.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e43efe-96f6-48b1-8f7e-489dfd04f85a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

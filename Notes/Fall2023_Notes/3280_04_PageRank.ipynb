{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b167603d-583e-48f6-a9c9-64cc1598d787",
   "metadata": {},
   "source": [
    "# 04 PageRank\n",
    "__Math 3280 - Data Mining__ : Snow College : Dr. Michael E. Olson\n",
    "\n",
    "* Leskovec, Chapter 5\n",
    "* [PageRank: Link Analysis Explanation and Python Implementation from Scratch, *Towards Data Science*](https://towardsdatascience.com/pagerank-3c568a7d2332)\n",
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c00811c5-3121-4926-bddc-b4c0d419778e",
   "metadata": {},
   "source": [
    "Early search engines\n",
    "* Programs would crawl through websites, listing terms (words or other strings of characters other than white space) used in that page.\n",
    "* The list of terms would be stored in an inverted index (a data structure that makes it easy, given a term, to find all places where that term occurs)\n",
    "* A search query would move through the inverted index and look for pages with the searched terms\n",
    "  * The result would be a ranked list of pages that use those terms\n",
    "\n",
    "Problems\n",
    "* __Term Spam__ refers to techniques for fooling search engines into believe your page is about something it is not\n",
    "  * To attract users to their website, some developers would take a list of key terms and add it to their webpage multiple times in a font the same color as the background so they'd get a high ranking in these early search engines\n",
    "  * Example, you have a website to sell GPS units, but want to attract anyone who is searching for local hikes\n",
    "    * You encode the webpage with the words \"local\" and \"hike\" repeated multiple times\n",
    "    * If your page is grey, then you change the font color of the repeated words to be the same shade of grey so they aren't immediately apparent to the user\n",
    "    * The more times the words \"local\" and \"hike\" appear in your page, the more likely your website will get a high rank in a search query\n",
    "    \n",
    "Two men worked on a new piece of software named __PageRank__ which combats term spam:\n",
    "* A search query creates __web surfers__, or random paths following pages link to link\n",
    "  * Website A has a link to website B and follows it, and finds on site B a link to site C, and so on, crawling from link to link through the web\n",
    "* Other components are also considered, such as how many sites link to that page\n",
    "* Calculating the probability that a site is at the end of the crawl gives a way to rank the sites\n",
    "\n",
    "We'll look at\n",
    "1. Strongly connected websites (no dead ends)\n",
    "2. Other components on the web that introduce dead ends\n",
    "3. Spider Traps\n",
    "4. Using taxation to resolve dead ends and spider traps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eebe8a12-88ae-42e2-be96-4bdc49ef06c0",
   "metadata": {},
   "source": [
    "## Strongly connected websites\n",
    "\n",
    "\n",
    "Figure 5.1\n",
    "* A -> B, C, D\n",
    "* B -> A, D\n",
    "* C -> A\n",
    "* D -> B, C\n",
    "\n",
    "Create a transition matrix ($M$)\n",
    "* Each column is a starting page (A, B, C, or D)\n",
    "* Each row is a linked page that is linked to from a starting page\n",
    "* The values are the probability that a linked page is selected from the given starting page\n",
    "  * $1/k$ where there are $k$ links on a page\n",
    "  * This process is __stochastic__\n",
    "    * There is an equal probability (random chances) for each page to be selected\n",
    "    * The total probability of each column is 1\n",
    "\n",
    "Starting vector $\\mathbf{v_0}$\n",
    "* A vector of size $n$ (the number of sites in the graph) whose elements are $1/n$, indicating the probability of that being a starting page\n",
    "\n",
    "A simulated web surfer's first step:\n",
    "$$v_1 = Mv_0$$\n",
    "\n",
    "The result of this multiplication is the probability of landing on each site after the first step. Find the probability of each following step:\n",
    "$$v_{i+1} = Mv_i$$\n",
    "\n",
    "If we complete this a large number of times, the vector will approach the probability of ending up on that particular site. The sites with the highest probabilities are listed first in the search query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46965442-45a9-4b7a-8aa6-5e62b7bef91e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.375      0.20833333 0.20833333 0.20833333]\n",
      "[0.3125     0.22916667 0.22916667 0.22916667]\n",
      "[0.34375 0.21875 0.21875 0.21875]\n",
      "[0.328125   0.22395833 0.22395833 0.22395833]\n",
      "[0.3359375  0.22135417 0.22135417 0.22135417]\n",
      "[0.33203125 0.22265625 0.22265625 0.22265625]\n",
      "[0.33398438 0.22200521 0.22200521 0.22200521]\n",
      "[0.33300781 0.22233073 0.22233073 0.22233073]\n",
      "[0.33349609 0.22216797 0.22216797 0.22216797]\n",
      "[0.33325195 0.22224935 0.22224935 0.22224935]\n",
      "[0.33337402 0.22220866 0.22220866 0.22220866]\n",
      "[0.33331299 0.222229   0.222229   0.222229  ]\n",
      "[0.33334351 0.22221883 0.22221883 0.22221883]\n",
      "[0.33332825 0.22222392 0.22222392 0.22222392]\n",
      "[0.33333588 0.22222137 0.22222137 0.22222137]\n",
      "[0.33333206 0.22222265 0.22222265 0.22222265]\n",
      "[0.33333397 0.22222201 0.22222201 0.22222201]\n",
      "[0.33333302 0.22222233 0.22222233 0.22222233]\n",
      "[0.33333349 0.22222217 0.22222217 0.22222217]\n",
      "[0.33333325 0.22222225 0.22222225 0.22222225]\n",
      "[0.33333337 0.22222221 0.22222221 0.22222221]\n",
      "[0.33333331 0.22222223 0.22222223 0.22222223]\n",
      "[0.33333334 0.22222222 0.22222222 0.22222222]\n",
      "[0.33333333 0.22222222 0.22222222 0.22222222]\n",
      "[0.33333334 0.22222222 0.22222222 0.22222222]\n",
      "[0.33333333 0.22222222 0.22222222 0.22222222]\n",
      "[0.33333333 0.22222222 0.22222222 0.22222222]\n",
      "[0.33333333 0.22222222 0.22222222 0.22222222]\n",
      "[0.33333333 0.22222222 0.22222222 0.22222222]\n",
      "[0.33333333 0.22222222 0.22222222 0.22222222]\n"
     ]
    }
   ],
   "source": [
    "# Transition Matrix\n",
    "import numpy as np\n",
    "#              Starting Page     A,   B, C,   D\n",
    "transition_matrix = np.array([[  0, 1/2, 1,   0],  # Linked page A\n",
    "                              [1/3,   0, 0, 1/2],  #             B\n",
    "                              [1/3,   0, 0, 1/2],  #             C\n",
    "                              [1/3, 1/2, 0,   0]]) #             D\n",
    "\n",
    "# Starting vector\n",
    "v = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "\n",
    "# Web surfer steps\n",
    "for i in range(30):\n",
    "    v = np.matmul(transition_matrix, v)\n",
    "    print(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "369ee01f-94e4-4e53-9849-c94230ba02c8",
   "metadata": {},
   "source": [
    "(Solution: [3/9, 2/9, 2/9, 2/9])\n",
    "\n",
    "## Dead ends\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Figure 5.3 - Same as figure 5.1, but remove the link from C to A\n",
    "* A -> B, C, D\n",
    "* B -> A, D\n",
    "* C -> __Dead End__\n",
    "* D -> B, C\n",
    "\n",
    "This process is no longer stochastic, since the total of column C is 0, not 1. Since some columns are stochastic, but not all, we call this __substochastic__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7489d531-1332-41bd-a80c-935ceeae3fad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.125      0.20833333 0.20833333 0.20833333]\n",
      "[0.10416667 0.14583333 0.14583333 0.14583333]\n",
      "[0.07291667 0.10763889 0.10763889 0.10763889]\n",
      "[0.05381944 0.078125   0.078125   0.078125  ]\n",
      "[0.0390625  0.05700231 0.05700231 0.05700231]\n",
      "[0.02850116 0.04152199 0.04152199 0.04152199]\n",
      "[0.020761   0.03026138 0.03026138 0.03026138]\n",
      "[0.01513069 0.02205102 0.02205102 0.02205102]\n",
      "[0.01102551 0.01606907 0.01606907 0.01606907]\n",
      "[0.00803454 0.01170971 0.01170971 0.01170971]\n",
      "[0.00585485 0.00853303 0.00853303 0.00853303]\n",
      "[0.00426652 0.00621813 0.00621813 0.00621813]\n",
      "[0.00310907 0.00453124 0.00453124 0.00453124]\n",
      "[0.00226562 0.00330198 0.00330198 0.00330198]\n",
      "[0.00165099 0.00240619 0.00240619 0.00240619]\n",
      "[0.0012031  0.00175343 0.00175343 0.00175343]\n",
      "[0.00087671 0.00127775 0.00127775 0.00127775]\n",
      "[0.00063887 0.00093111 0.00093111 0.00093111]\n",
      "[0.00046556 0.00067851 0.00067851 0.00067851]\n",
      "[0.00033926 0.00049444 0.00049444 0.00049444]\n",
      "[0.00024722 0.00036031 0.00036031 0.00036031]\n",
      "[0.00018015 0.00026256 0.00026256 0.00026256]\n",
      "[0.00013128 0.00019133 0.00019133 0.00019133]\n",
      "[9.56655281e-05 1.39425534e-04 1.39425534e-04 1.39425534e-04]\n",
      "[6.97127669e-05 1.01601276e-04 1.01601276e-04 1.01601276e-04]\n",
      "[5.08006382e-05 7.40382271e-05 7.40382271e-05 7.40382271e-05]\n",
      "[3.70191136e-05 5.39526596e-05 5.39526596e-05 5.39526596e-05]\n",
      "[2.69763298e-05 3.93160343e-05 3.93160343e-05 3.93160343e-05]\n",
      "[1.96580172e-05 2.86501271e-05 2.86501271e-05 2.86501271e-05]\n",
      "[1.43250636e-05 2.08777359e-05 2.08777359e-05 2.08777359e-05]\n",
      "[1.04388680e-05 1.52138892e-05 1.52138892e-05 1.52138892e-05]\n",
      "[7.60694458e-06 1.10865672e-05 1.10865672e-05 1.10865672e-05]\n",
      "[5.54328362e-06 8.07893181e-06 8.07893181e-06 8.07893181e-06]\n",
      "[4.03946590e-06 5.88722711e-06 5.88722711e-06 5.88722711e-06]\n",
      "[2.94361356e-06 4.29010219e-06 4.29010219e-06 4.29010219e-06]\n",
      "[2.14505109e-06 3.12625561e-06 3.12625561e-06 3.12625561e-06]\n",
      "[1.56312781e-06 2.27814484e-06 2.27814484e-06 2.27814484e-06]\n",
      "[1.13907242e-06 1.66011502e-06 1.66011502e-06 1.66011502e-06]\n",
      "[8.30057511e-07 1.20974832e-06 1.20974832e-06 1.20974832e-06]\n",
      "[6.04874159e-07 8.81559995e-07 8.81559995e-07 8.81559995e-07]\n",
      "[4.40779998e-07 6.42404717e-07 6.42404717e-07 6.42404717e-07]\n",
      "[3.21202359e-07 4.68129025e-07 4.68129025e-07 4.68129025e-07]\n",
      "[2.34064512e-07 3.41131965e-07 3.41131965e-07 3.41131965e-07]\n",
      "[1.70565983e-07 2.48587487e-07 2.48587487e-07 2.48587487e-07]\n",
      "[1.24293743e-07 1.81149071e-07 1.81149071e-07 1.81149071e-07]\n",
      "[9.05745354e-08 1.32005783e-07 1.32005783e-07 1.32005783e-07]\n",
      "[6.60028916e-08 9.61944034e-08 9.61944034e-08 9.61944034e-08]\n",
      "[4.80972017e-08 7.00981656e-08 7.00981656e-08 7.00981656e-08]\n",
      "[3.50490828e-08 5.10814834e-08 5.10814834e-08 5.10814834e-08]\n",
      "[2.55407417e-08 3.72237693e-08 3.72237693e-08 3.72237693e-08]\n",
      "[1.86118846e-08 2.71254652e-08 2.71254652e-08 2.71254652e-08]\n",
      "[1.35627326e-08 1.97666941e-08 1.97666941e-08 1.97666941e-08]\n",
      "[9.88334707e-09 1.44042579e-08 1.44042579e-08 1.44042579e-08]\n",
      "[7.20212897e-09 1.04965780e-08 1.04965780e-08 1.04965780e-08]\n",
      "[5.24828900e-09 7.64899865e-09 7.64899865e-09 7.64899865e-09]\n",
      "[3.82449933e-09 5.57392899e-09 5.57392899e-09 5.57392899e-09]\n",
      "[2.7869645e-09 4.0617976e-09 4.0617976e-09 4.0617976e-09]\n",
      "[2.03089880e-09 2.95988697e-09 2.95988697e-09 2.95988697e-09]\n",
      "[1.47994348e-09 2.15690975e-09 2.15690975e-09 2.15690975e-09]\n",
      "[1.07845488e-09 1.57176937e-09 1.57176937e-09 1.57176937e-09]\n"
     ]
    }
   ],
   "source": [
    "#              Starting Page     A,   B, C,   D\n",
    "transition_matrix = np.array([[  0, 1/2, 0,   0],  # Linked page A\n",
    "                              [1/3,   0, 0, 1/2],  #             B\n",
    "                              [1/3,   0, 0, 1/2],  #             C\n",
    "                              [1/3, 1/2, 0,   0]]) #             D\n",
    "\n",
    "# Starting vector\n",
    "v = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "\n",
    "# Web surfer steps\n",
    "for i in range(60):\n",
    "    v = np.matmul(transition_matrix, v)\n",
    "    print(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0085da78-9af9-4587-9163-f8ed26b360da",
   "metadata": {},
   "source": [
    "(Solution after a large number of iterations: [0,0,0,0])\n",
    "\n",
    "Two approaches to dealing with dead ends\n",
    "1. Drop all dead ends\n",
    "   * Will also need to drop sites that only lead to dead ends\n",
    "   * If site J leads only to K, and K is a dead end, then K is dropped. J then become a dead end, so will also need to be dropped\n",
    "2. Modify the process by which random surfers are assumed to move about the web (the modification we'll look at is \"taxation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61741dfe-0348-42e6-a04d-e9531daa1d17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16666667 0.5        0.33333333]\n",
      "[0.25       0.41666667 0.33333333]\n",
      "[0.20833333 0.45833333 0.33333333]\n",
      "[0.22916667 0.4375     0.33333333]\n",
      "[0.21875    0.44791667 0.33333333]\n",
      "[0.22395833 0.44270833 0.33333333]\n",
      "[0.22135417 0.4453125  0.33333333]\n",
      "[0.22265625 0.44401042 0.33333333]\n",
      "[0.22200521 0.44466146 0.33333333]\n",
      "[0.22233073 0.44433594 0.33333333]\n",
      "[0.22216797 0.4444987  0.33333333]\n",
      "[0.22224935 0.44441732 0.33333333]\n",
      "[0.22220866 0.44445801 0.33333333]\n",
      "[0.222229   0.44443766 0.33333333]\n",
      "[0.22221883 0.44444784 0.33333333]\n",
      "[0.22222392 0.44444275 0.33333333]\n",
      "[0.22222137 0.44444529 0.33333333]\n",
      "[0.22222265 0.44444402 0.33333333]\n",
      "[0.22222201 0.44444466 0.33333333]\n",
      "[0.22222233 0.44444434 0.33333333]\n",
      "[0.22222217 0.4444445  0.33333333]\n",
      "[0.22222225 0.44444442 0.33333333]\n",
      "[0.22222221 0.44444446 0.33333333]\n",
      "[0.22222223 0.44444444 0.33333333]\n",
      "[0.22222222 0.44444445 0.33333333]\n",
      "[0.22222222 0.44444444 0.33333333]\n",
      "[0.22222222 0.44444445 0.33333333]\n",
      "[0.22222222 0.44444444 0.33333333]\n",
      "[0.22222222 0.44444444 0.33333333]\n",
      "[0.22222222 0.44444444 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "#              Starting Page     A,   B, D\n",
    "transition_matrix = np.array([[  0, 1/2, 0],  # Linked page A\n",
    "                              [1/2,   0, 1],  #             B\n",
    "                              [1/2, 1/2, 0]]) #             D\n",
    "\n",
    "# Starting vector\n",
    "v = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "# Web surfer steps\n",
    "for i in range(30):\n",
    "    v = np.matmul(transition_matrix, v)\n",
    "    print(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "166bef68-3beb-48d2-9dc5-bbf4af79b6a4",
   "metadata": {},
   "source": [
    "(Solution: [2/9, 4/9, 3/9])\n",
    "\n",
    "## Spider traps\n",
    "Occasionally, a website will link to itself (e.g. a website links to a section halfway down the page, another link returns the user to the top of the page). If the only links on a website are to itself, then it is called a __spider trap__. As there are links, it is not a dead end. Although this may make sense as a web developer, it does mess up the PageRank algorithm.\n",
    "\n",
    "\n",
    "\n",
    "Figure 5.6 - Same as figure 5.3, but C has a spider trap (link to itself)\n",
    "* A -> B, C, D\n",
    "* B -> A, D\n",
    "* C -> C\n",
    "* D -> B, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850d23d5-a96d-4bce-a121-9275425ed301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.125      0.20833333 0.45833333 0.20833333]\n",
      "[0.10416667 0.14583333 0.60416667 0.14583333]\n",
      "[0.07291667 0.10763889 0.71180556 0.10763889]\n",
      "[0.05381944 0.078125   0.78993056 0.078125  ]\n",
      "[0.0390625  0.05700231 0.84693287 0.05700231]\n",
      "[0.02850116 0.04152199 0.88845486 0.04152199]\n",
      "[0.020761   0.03026138 0.91871624 0.03026138]\n",
      "[0.01513069 0.02205102 0.94076726 0.02205102]\n",
      "[0.01102551 0.01606907 0.95683634 0.01606907]\n",
      "[0.00803454 0.01170971 0.96854605 0.01170971]\n",
      "[0.00585485 0.00853303 0.97707908 0.00853303]\n",
      "[0.00426652 0.00621813 0.98329721 0.00621813]\n",
      "[0.00310907 0.00453124 0.98782845 0.00453124]\n",
      "[0.00226562 0.00330198 0.99113043 0.00330198]\n",
      "[0.00165099 0.00240619 0.99353662 0.00240619]\n",
      "[0.0012031  0.00175343 0.99529005 0.00175343]\n",
      "[8.76713192e-04 1.27774557e-03 9.96567796e-01 1.27774557e-03]\n",
      "[6.38872786e-04 9.31110517e-04 9.97498906e-01 9.31110517e-04]\n",
      "[4.65555258e-04 6.78512854e-04 9.98177419e-01 6.78512854e-04]\n",
      "[3.39256427e-04 4.94441513e-04 9.98671861e-01 4.94441513e-04]\n",
      "[2.47220757e-04 3.60306232e-04 9.99032167e-01 3.60306232e-04]\n",
      "[1.80153116e-04 2.62560035e-04 9.99294727e-01 2.62560035e-04]\n",
      "[1.31280017e-04 1.91331056e-04 9.99486058e-01 1.91331056e-04]\n",
      "[9.56655281e-05 1.39425534e-04 9.99625483e-01 1.39425534e-04]\n",
      "[6.97127669e-05 1.01601276e-04 9.99727085e-01 1.01601276e-04]\n",
      "[5.08006382e-05 7.40382271e-05 9.99801123e-01 7.40382271e-05]\n",
      "[3.70191136e-05 5.39526596e-05 9.99855076e-01 5.39526596e-05]\n",
      "[2.69763298e-05 3.93160343e-05 9.99894392e-01 3.93160343e-05]\n",
      "[1.96580172e-05 2.86501271e-05 9.99923042e-01 2.86501271e-05]\n",
      "[1.43250636e-05 2.08777359e-05 9.99943919e-01 2.08777359e-05]\n",
      "[1.04388680e-05 1.52138892e-05 9.99959133e-01 1.52138892e-05]\n",
      "[7.60694458e-06 1.10865672e-05 9.99970220e-01 1.10865672e-05]\n",
      "[5.54328362e-06 8.07893181e-06 9.99978299e-01 8.07893181e-06]\n",
      "[4.03946590e-06 5.88722711e-06 9.99984186e-01 5.88722711e-06]\n",
      "[2.94361356e-06 4.29010219e-06 9.99988476e-01 4.29010219e-06]\n",
      "[2.14505109e-06 3.12625561e-06 9.99991602e-01 3.12625561e-06]\n",
      "[1.56312781e-06 2.27814484e-06 9.99993881e-01 2.27814484e-06]\n",
      "[1.13907242e-06 1.66011502e-06 9.99995541e-01 1.66011502e-06]\n",
      "[8.30057511e-07 1.20974832e-06 9.99996750e-01 1.20974832e-06]\n",
      "[6.04874159e-07 8.81559995e-07 9.99997632e-01 8.81559995e-07]\n",
      "[4.40779998e-07 6.42404717e-07 9.99998274e-01 6.42404717e-07]\n",
      "[3.21202359e-07 4.68129025e-07 9.99998743e-01 4.68129025e-07]\n",
      "[2.34064512e-07 3.41131965e-07 9.99999084e-01 3.41131965e-07]\n",
      "[1.70565983e-07 2.48587487e-07 9.99999332e-01 2.48587487e-07]\n",
      "[1.24293743e-07 1.81149071e-07 9.99999513e-01 1.81149071e-07]\n",
      "[9.05745354e-08 1.32005783e-07 9.99999645e-01 1.32005783e-07]\n",
      "[6.60028916e-08 9.61944034e-08 9.99999742e-01 9.61944034e-08]\n",
      "[4.80972017e-08 7.00981656e-08 9.99999812e-01 7.00981656e-08]\n",
      "[3.50490828e-08 5.10814834e-08 9.99999863e-01 5.10814834e-08]\n",
      "[2.55407417e-08 3.72237693e-08 9.99999900e-01 3.72237693e-08]\n",
      "[1.86118846e-08 2.71254652e-08 9.99999927e-01 2.71254652e-08]\n",
      "[1.35627326e-08 1.97666941e-08 9.99999947e-01 1.97666941e-08]\n",
      "[9.88334707e-09 1.44042579e-08 9.99999961e-01 1.44042579e-08]\n",
      "[7.20212897e-09 1.04965780e-08 9.99999972e-01 1.04965780e-08]\n",
      "[5.24828900e-09 7.64899865e-09 9.99999979e-01 7.64899865e-09]\n",
      "[3.82449933e-09 5.57392899e-09 9.99999985e-01 5.57392899e-09]\n",
      "[2.78696450e-09 4.06179760e-09 9.99999989e-01 4.06179760e-09]\n",
      "[2.03089880e-09 2.95988697e-09 9.99999992e-01 2.95988697e-09]\n",
      "[1.47994348e-09 2.15690975e-09 9.99999994e-01 2.15690975e-09]\n",
      "[1.07845488e-09 1.57176937e-09 9.99999996e-01 1.57176937e-09]\n"
     ]
    }
   ],
   "source": [
    "#              Starting Page     A,   B, C,   D\n",
    "transition_matrix = np.array([[  0, 1/2, 0,   0],  # Linked page A\n",
    "                              [1/3,   0, 0, 1/2],  #             B\n",
    "                              [1/3,   0, 1, 1/2],  #             C\n",
    "                              [1/3, 1/2, 0,   0]]) #             D\n",
    "\n",
    "# Starting vector\n",
    "v = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "\n",
    "# Web surfer steps\n",
    "for i in range(60):\n",
    "    v = np.matmul(transition_matrix, v)\n",
    "    print(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b701cfd-0043-481f-b5f5-3b18862ad8d0",
   "metadata": {},
   "source": [
    "(Solution: [0,0,1,0])\n",
    "\n",
    "The result is a 100% chance of ending up in the spider trap."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cd35eac-17ec-469a-9adc-59c8bbc3884d",
   "metadata": {},
   "source": [
    "## Taxation\n",
    "To resolve this, we use a process called __taxation__. We modify the PageRank algorithm by allowing each random surfer a small probability of *teleporting* to a random page. If there is a probability $\\beta$ (taxation parameter) that a surfer is not teleported, then our random surf equation becomes,\n",
    "$$\\mathbf{v}' = \\beta M\\mathbf{v} + \\frac{1-\\beta}{n}\\mathbf{e}$$\n",
    "\n",
    "where $\\mathbf{e}$ is a vector of 1's the same size as $\\mathbf{v}$ and where $n$ is the size of the vector $\\mathbf{v}$ (or the number of webpages in question). The term on the right increases the probabilty of randomly ending up on another page (e.g. the user stops a current line of search and goes somewhere else). Generally, the probability of not being transported is set between $\\beta=70$ and $\\beta=90$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41833363-8696-44f6-a002-e1145f069a8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35       0.21666667 0.21666667 0.21666667]\n",
      "[0.31 0.23 0.23 0.23]\n",
      "[0.326      0.22466667 0.22466667 0.22466667]\n",
      "[0.3196 0.2268 0.2268 0.2268]\n",
      "[0.32216    0.22594667 0.22594667 0.22594667]\n",
      "[0.321136 0.226288 0.226288 0.226288]\n",
      "[0.3215456  0.22615147 0.22615147 0.22615147]\n",
      "[0.32138176 0.22620608 0.22620608 0.22620608]\n",
      "[0.3214473  0.22618423 0.22618423 0.22618423]\n",
      "[0.32142108 0.22619297 0.22619297 0.22619297]\n",
      "[0.32143157 0.22618948 0.22618948 0.22618948]\n",
      "[0.32142737 0.22619088 0.22619088 0.22619088]\n",
      "[0.32142905 0.22619032 0.22619032 0.22619032]\n",
      "[0.32142838 0.22619054 0.22619054 0.22619054]\n",
      "[0.32142865 0.22619045 0.22619045 0.22619045]\n",
      "[0.32142854 0.22619049 0.22619049 0.22619049]\n",
      "[0.32142858 0.22619047 0.22619047 0.22619047]\n",
      "[0.32142857 0.22619048 0.22619048 0.22619048]\n",
      "[0.32142857 0.22619048 0.22619048 0.22619048]\n",
      "[0.32142857 0.22619048 0.22619048 0.22619048]\n"
     ]
    }
   ],
   "source": [
    "#              Starting Page     A,   B, C,   D\n",
    "transition_matrix = np.array([[  0, 1/2, 1,   0],  # Linked page A\n",
    "                              [1/3,   0, 0, 1/2],  #             B\n",
    "                              [1/3,   0, 0, 1/2],  #             C\n",
    "                              [1/3, 1/2, 0,   0]]) #             D\n",
    "\n",
    "# Starting vector\n",
    "v = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "\n",
    "# Teleporting vector\n",
    "e = np.array([1, 1, 1, 1])\n",
    "beta = 0.8\n",
    "\n",
    "# Web surfer steps\n",
    "for i in range(20):\n",
    "    v = beta*np.matmul(transition_matrix, v) + e*(1-beta)/len(e)\n",
    "    print(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "240b4e1b-f23e-470d-be7c-85a5eb785dab",
   "metadata": {},
   "source": [
    "(These solutions are [15/148, 19/148, 95/148, 19/148])\n",
    "\n",
    "We have looked at how PageRank is used based on links to other pages. Every search engine has a more complicated algorithm based on more components:\n",
    "* Number of pages with links to that page\n",
    "* Frequency of words used\n",
    "* Relevance of word usage based on words around it (Natural Language Processing)\n",
    "* ...etc...\n",
    "\n",
    "Google is said to have over 250 different components to their algorithm. Normally, the weighting of properties is such that unless all the search terms are present, a page has very little chance of being high on the list."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23ef04c4-8f12-4292-9d1d-8d5496b07990",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "915aca3a-d9a1-4d8f-b527-306cc7221735",
   "metadata": {},
   "source": [
    "## 5.2 Efficient Computation of PageRank\n",
    "The real web is far more complicated than the example we just did. There are, however, two major problems,\n",
    "1. With billions of websites ($n$), the matrix will be $n\\times n$, with over $10^{18}$ elements. It would be a very sparse matrix since no website links to more than a handful of websites.\n",
    "2. We could use MapReduce to do the calculation, but the basic approaches are not sufficient to avoid heavy use.\n",
    "\n",
    "### Representing the matrix\n",
    "A matrix representing $n$ websites will take $n^2$ bytes. However, if we use locality-sensitive hashing, then we don't need to store the entire (sparse) matrix. In fact, the space needed for the relevant information in the matrix will no longer be quadratic, but a linear relationship to the number of links involved.\n",
    "\n",
    "In addition, every value for a link within a page will have the same value: 1 divided by the out-degree of the page (the number of links on the page). Using the links from Figure 5.1,\n",
    "\n",
    "$$M=\\begin{bmatrix}\n",
    "  0 & 1/2 & 1 & 0 \\\\\n",
    "  1/3 & 0 & 0 & 1/2 \\\\\n",
    "  1/3 & 0 & 0 & 1/2 \\\\\n",
    "  1/3 & 1/2 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "The matrix $M$ has 16 elements, so it would take 16 bytes of memory. But if we use LSH, \n",
    "\n",
    "| Source | Degree | Value | Destinations |\n",
    "| ------ | ------ | ----- | ------------ |\n",
    "| A      | 3      | 1/3   | B,C,D        |\n",
    "| B      | 2      | 1/2   | A,D          |\n",
    "| C      | 1      | 1     | A            |\n",
    "| D      | 2      | 1/2   | B,C          |\n",
    "\n",
    "The LSH list would be:\n",
    "* (B, 1/3)\n",
    "* (C, 1/3)\n",
    "* (D, 1/3)\n",
    "* (A, 1/2)\n",
    "* (B, 1/2)\n",
    "* (C, 1)\n",
    "* (B, 1/2)\n",
    "* (C, 1/2)\n",
    "\n",
    "That is 8 entries. If each takes 4 bytes, then that is 32 bytes of memory. For this example, the LSH takes up more space. However, if we add two more sites, $M$ increases to 36 bytes, and LSH only increases by the number of links on the two pages (if there are 2 links on each page, it goes up to 36 bytes)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fe61a07-ad0f-4a40-8cbc-c7d894cbcc64",
   "metadata": {},
   "source": [
    "## 5.3 Topic-Sensitive PageRank\n",
    "Currently, using the PageRank with teleporting, there is an equal probability that the web surfer ends up at any page. But could we make this even better? For example, if we take the term *Diamondback*, I could be referring to one of the following:\n",
    "* The Diamondback rattlesnake (animal)\n",
    "* A Diamondback Mountain Bike (recreation)\n",
    "* The Arizona Diamondbacks MLB team (sports)\n",
    "\n",
    "If the system knows a little more about the user, perhaps the PageRank could give better results.\n",
    "\n",
    "All topics for websites can be divided into specific categories. One such useful set of 16 top-level categories is known as the DMOZ. Once we know the topics of each website, then we can weight the teleport for each site. Let's set $S$ to be the weighted teleport set, and $\\mathbf{e}_S$ the vector that has 1 for each element in $S$ and 0 for all other elements. We calculate the __Topic-Sensitive PageRank__ as:\n",
    "$$\\mathbf{v}' = \\beta M\\mathbf{v} + \\frac{1-\\beta}{|S|}\\mathbf{e_S}$$\n",
    "\n",
    "Let us say that $S=\\{B,D\\}$, meaning the only sites teleported to are sites $B$ and $D$. Let's also leave the taxation parameter as $\\beta=0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d696383-ea6e-464a-beda-85d92a8d10e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.3 0.2 0.3]\n",
      "[0.28       0.27333333 0.17333333 0.27333333]\n",
      "[0.248 0.284 0.184 0.284]\n",
      "[0.2608     0.27973333 0.17973333 0.27973333]\n",
      "[0.25568 0.28144 0.18144 0.28144]\n",
      "[0.257728   0.28075733 0.18075733 0.28075733]\n",
      "[0.2569088 0.2810304 0.1810304 0.2810304]\n",
      "[0.25723648 0.28092117 0.18092117 0.28092117]\n",
      "[0.25710541 0.28096486 0.18096486 0.28096486]\n",
      "[0.25715784 0.28094739 0.18094739 0.28094739]\n",
      "[0.25713687 0.28095438 0.18095438 0.28095438]\n",
      "[0.25714525 0.28095158 0.18095158 0.28095158]\n",
      "[0.2571419 0.2809527 0.1809527 0.2809527]\n",
      "[0.25714324 0.28095225 0.18095225 0.28095225]\n",
      "[0.2571427  0.28095243 0.18095243 0.28095243]\n",
      "[0.25714292 0.28095236 0.18095236 0.28095236]\n",
      "[0.25714283 0.28095239 0.18095239 0.28095239]\n",
      "[0.25714287 0.28095238 0.18095238 0.28095238]\n",
      "[0.25714285 0.28095238 0.18095238 0.28095238]\n",
      "[0.25714286 0.28095238 0.18095238 0.28095238]\n"
     ]
    }
   ],
   "source": [
    "#              Starting Page     A,   B, C,   D\n",
    "transition_matrix = np.array([[  0, 1/2, 1,   0],  # Linked page A\n",
    "                              [1/3,   0, 0, 1/2],  #             B\n",
    "                              [1/3,   0, 0, 1/2],  #             C\n",
    "                              [1/3, 1/2, 0,   0]]) #             D\n",
    "\n",
    "# Starting vector\n",
    "v = np.array([0, 1/2, 0, 1/2])\n",
    "\n",
    "# Teleporting vector\n",
    "es = np.array([0, 1, 0, 1])\n",
    "beta = 0.8\n",
    "\n",
    "# Web surfer steps\n",
    "for i in range(20):\n",
    "    v = beta*np.matmul(transition_matrix, v) + es*(1-beta)/sum(es)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4c7ff-ff6f-40c0-824b-9b55c78bf80b",
   "metadata": {},
   "source": [
    "$$\\require{cancel}$$ \n",
    "### 5.4 Link Spam\n",
    "We talked earlier about term spam, where spammers add specific words to their webpage to increase the search results. We effectively combated that by looking at links to each page using PageRank. However, there are groups that have found a way around PageRank. This is called __link spam__ which increases the PageRank of a spam page by use of a series of sites called a __spam farm__.\n",
    "\n",
    "Spammers are dealing with three different types of pages:\n",
    "1. Inaccessible pages\n",
    "    * Spammers can do nothing about these pages\n",
    "    * Generally don't link to spam pages\n",
    "2. Accessible pages\n",
    "    * Spammers can't change the page, but can manipulate them\n",
    "    * For example, on a blog, the spammer can't change what's on the page, but they can add a comment to the effect of \"Good points! I have some additional comments at [link to spam page].\"\n",
    "3. Own pages\n",
    "    * A series of pages that the spammer owns and controls, adding links to their target page(s)\n",
    "  \n",
    "To analyze a spam farm, let's define the following:\n",
    "* $t$: The spammer's target page\n",
    "* $x$: The PageRank of $t$ from Accessible Pages\n",
    "* $m$: The number of pages in the spam farm\n",
    "* $y$: Total (unknown) PageRank for the target page $t$\n",
    "\n",
    "We can calculate the PageRank for $t$ from a single page in the Spam Farm as the probability of being directed to $t$ from any page $z_i$ within the spam farm as ($\\beta y/m$) plus the probability of being teleported to that page ($(1-\\beta)/n$).\n",
    "$$z_i = \\frac{\\beta y}{m} + \\frac{1-\\beta}{n}$$\n",
    "\n",
    "The contribution from the entire spam farm is then $z=mz_i$\n",
    "\n",
    "We can now calculate the total PageRank for $t$:\n",
    "1. There is no contribution from Inaccessible Pages\n",
    "2. The contribution from Accessible Pages is simply $x$\n",
    "3. The contribution from the Spam Farm is $\\beta z + (1-\\beta)/n$\n",
    "    * The last term $(1-\\beta)/n$ is so small that it is relatively insignificant\n",
    "$$y = x + \\beta z + \\cancel{\\frac{1-\\beta}{n}}= x + \\beta m\\left(\\frac{\\beta y}{m} + \\frac{1-\\beta}{n}\\right) + \\cancel{\\frac{1-\\beta}{n}}$$\n",
    "$$y = x + \\beta^2 y + \\beta(1-\\beta)\\frac{m}{n}$$\n",
    "\n",
    "Solving for $y$,\n",
    "$$y - \\beta^2 y = x + \\beta(1-\\beta)\\frac{m}{n}$$\n",
    "$$y = \\frac{x}{1-\\beta^2} + \\frac{\\beta(1-\\beta)}{1-\\beta^2}\\frac{m}{n}$$\n",
    "$$y = \\frac{x}{1-\\beta^2} + \\frac{\\beta(1-\\beta)}{(1-\\beta)(1+\\beta)}\\frac{m}{n}$$\n",
    "$$y = \\frac{x}{1-\\beta^2} + \\frac{\\beta}{1+\\beta}\\frac{m}{n}$$\n",
    "\n",
    "Thus we see that the PageRank contribution from Accessible Pages is $x/(1-\\beta^2)$ and the contribution from the spam farm is proportional to the ratio of the farm's size to the entire internet $(m/n)$.\n",
    "\n",
    "__Example__: If we are calculating the PageRank of a $link spam$ page using a taxation parameter of $\\beta = 0.85$, then the contribution from accessible pages $x$ increases by a factor of,\n",
    "$$\\frac{1}{1-\\beta^2} = 3.60 = 360\\%$$\n",
    "\n",
    "while the contribution from the spam farm itself is the ratio $m/n$, increased by the factor,\n",
    "$$\\frac{\\beta}{1+\\beta} = 0.46 = 46\\%$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90941dca-5f02-4249-8b45-cd0bfc581bcf",
   "metadata": {},
   "source": [
    "### Combating Link Spam\n",
    "We have found that using PageRank discourages the use of term spam. How can we combat link spam? We do this simply with two tools:\n",
    "1. TrustRank\n",
    "2. Spam Mass\n",
    "\n",
    "__TrustRank__ is simply a topic-sensitive PageRank, but the set of pages is set to a set of *trusted* pages.\n",
    "* The likelihood of a trusted page linking to a spam page is very small\n",
    "* Two common approaches for determining trusted pages:\n",
    "   1. Humans examine pages and determine if they are trustworthy or not\n",
    "       * Requires a lot of hands-on work, which means pages are sent in small batches to people\n",
    "   2. Picking a domain whose membership is controlled (.edu, .mil, .gov, .ac.il, .edu.sg)\n",
    " \n",
    "Two major issues with TrustRank are (1) building the trusted set by human inspection requires a lot of work, so can only be done in small batches, and (2) all good pages need to somehow be reachable from the trusted set, which isn't always the case. The bottom line is that TrustRank is very effective at filtering out link spam, but also filters out valid but less common webpages in the meantime.\n",
    "\n",
    "The idea of __Spam Mass__ is to calculate the \"percentage\" of the PageRank that is from spam. If we assume the PageRank is a combination of TrustRank ($t$) and spam, then the PageRank ($r$) is the sum of the two. Thus, the contribution from spam is $r-t$. The percentage of the PageRank from spam is then,\n",
    "$$Spam~Mass = \\frac{r-t}{r} = 1 - \\frac{t}{r}$$\n",
    "\n",
    "Now, a percentage will be a number between 0 and 1. However, the TrustRank can be larger than the PageRank, which will give a negative number. So,\n",
    "* If the Spam Mass is negative, it is a trusted site\n",
    "* If the Spam Mass is close to 0, it has a low chance of being a trusted site\n",
    "* If the Spam Mass is close to 1, it has a very low TrustRank score, so is likely spam\n",
    "\n",
    "The following three cells use Figure 5.1 to calculate the PageRank, the TrustRank (using $B$ and $D$ as trusted pages), and the Spam Mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f630fcf1-6e21-490d-a937-41ae28c2c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PageRank ###\n",
    "# Transition Matrix\n",
    "import numpy as np\n",
    "#              Starting Page     A,   B, C,   D\n",
    "transition_matrix = np.array([[  0, 1/2, 1,   0],  # Linked page A\n",
    "                              [1/3,   0, 0, 1/2],  #             B\n",
    "                              [1/3,   0, 0, 1/2],  #             C\n",
    "                              [1/3, 1/2, 0,   0]]) #             D\n",
    "\n",
    "# Starting vector\n",
    "pagerank = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "\n",
    "# Web surfer steps\n",
    "for i in range(30):\n",
    "    pagerank = np.matmul(transition_matrix, pagerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e44c72b-9f80-4e9c-9e9f-ffb9783b8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TrustRank ###\n",
    "# Starting vector\n",
    "trustrank = np.array([0, 1/2, 0, 1/2])\n",
    "\n",
    "# Teleporting vector\n",
    "es = np.array([0, 1, 0, 1])\n",
    "beta = 0.8\n",
    "\n",
    "# Web surfer steps\n",
    "for i in range(20):\n",
    "    trustrank = beta*np.matmul(transition_matrix, trustrank) + es*(1-beta)/sum(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f70759-7a00-4dd0-bfd1-29ede77964c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22857142, -0.26428571,  0.18571429, -0.26428571])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Spam Mass ###\n",
    "1 - (trustrank/pagerank)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea3b64d2-219e-4075-9bdf-1d884ec1d9b1",
   "metadata": {},
   "source": [
    "* Since $B$ and $D$ were trusted pages, their scores are negative\n",
    "* $A$ and $C$ are linked to $B$ and $D$, so their Spam Masses are small\n",
    "* If a website $E$ were to be introduced that is not connected to $B$ and $D$, it would likely have a spam mass closer to 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa1764eb-5859-4bd7-a595-75c199f0231c",
   "metadata": {},
   "source": [
    "-----\n",
    "## Homework\n",
    "* Exercise 5.1.1\n",
    "* Exercise 5.1.2\n",
    "* Exercise 5.1.7\n",
    "* Exercise 5.2.1\n",
    "* Exercise 5.3.1 - Use $\\beta=0.82$\n",
    "* Exercise 5.4.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
